{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <b><i> Testing LightFM </i> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # > Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from lightfm import LightFM\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from scipy.sparse import identity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # > Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = \"1561003029.019894\"\n",
    "CHOSEN_EPOCH = 600\n",
    "\n",
    "MODEL_PATH = \"../log/{}/models/epoch_{}\".format(TEST_CODE, CHOSEN_EPOCH)\n",
    "LOG_PATH = \"../log/{}/log.txt\".format(TEST_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # > Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pivot_csr_filename = \"../data/intersect-20m/ratings.csr\"\n",
    "\n",
    "ratings_pivot = pickle.load(open(ratings_pivot_csr_filename, 'rb'))\n",
    "train, test = random_train_test_split(ratings_pivot, test_percentage=0.2)\n",
    "\n",
    "train_csr = train.tocsr()\n",
    "test_csr = test.tocsr()\n",
    "test_user, test_item = test.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(MODEL_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Users & items feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_identity = identity(train.shape[0])\n",
    "item_identity = identity(train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # > Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_suggestion(sample_user, k):\n",
    "\n",
    "    test_item_idx = [i for i in range(0, test.shape[1])]\n",
    "\n",
    "    prediction = model.predict(user_ids=sample_user, item_ids=test_item_idx, user_features=user_identity, item_features=item_identity)\n",
    "    prediction = [(prediction[i], i) for i in range(0, len(prediction))]\n",
    "    prediction = sorted(prediction, reverse=True)\n",
    "\n",
    "    return prediction[:k]\n",
    "\n",
    "\n",
    "def get_top_truth(sample_user, k):\n",
    "\n",
    "    truth = []\n",
    "\n",
    "    user_ratings = test_csr[sample_user].todense().tolist()[0]\n",
    "    user_rated_item = test_csr[sample_user].nonzero()[1]\n",
    "    for item in user_rated_item:\n",
    "        truth.append((user_ratings[item], item))\n",
    "\n",
    "    user_ratings = train_csr[sample_user].todense().tolist()[0]\n",
    "    user_rated_item = train_csr[sample_user].nonzero()[1]\n",
    "    for item in user_rated_item:\n",
    "        truth.append((user_ratings[item], item))\n",
    "\n",
    "    truth = sorted(truth, reverse=True)\n",
    "\n",
    "    return truth[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_intersect_pred_truth(prediction, truth, k):\n",
    "    pred_item_set = {x[1] for x in prediction[:k]}\n",
    "    truth_item_set = {x[1] for x in truth[:k]}\n",
    "\n",
    "    return pred_item_set.intersection(truth_item_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_precision(prediction, truth, k=10):\n",
    "\n",
    "    intersect = _get_intersect_pred_truth(prediction, truth, k)\n",
    "    len_intersect = len(intersect)\n",
    "    len_truth = len(truth) if 0 < len(truth) <= k else k\n",
    "\n",
    "    return intersect, len_intersect / len_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## >> Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:41<00:00, 30.97it/s]\n"
     ]
    }
   ],
   "source": [
    "k_suggestion = 10\n",
    "n_users = 5000\n",
    "\n",
    "sample_user = np.random.randint(1, 135000, n_users) # sampling\n",
    "# sample_user = [i in range(0, 15000)] # uncomment to use non sampling\n",
    "\n",
    "suggested_items = []\n",
    "truth_items = []\n",
    "intersects = []\n",
    "scores = []\n",
    "\n",
    "all_intersect = None\n",
    "all_union = None\n",
    "\n",
    "for user in tqdm(sample_user):\n",
    "\n",
    "    try:\n",
    "\n",
    "        top_suggestions = get_top_suggestion(user, k_suggestion)\n",
    "        top_suggested_items = set([x[1] for x in top_suggestions])\n",
    "        top_truth_items = get_top_truth(user, k_suggestion)\n",
    "\n",
    "        intersect, score = check_precision(top_suggestions, top_truth_items, k=k_suggestion)\n",
    "\n",
    "        suggested_items.append(top_suggested_items)\n",
    "        truth_items.append(top_truth_items)\n",
    "        intersects.append(intersect)\n",
    "        scores.append(score)\n",
    "\n",
    "        if all_intersect is None:\n",
    "            all_intersect = top_suggested_items\n",
    "        else:\n",
    "            all_intersect = all_intersect.intersection(top_suggested_items)\n",
    "\n",
    "        if all_union is None:\n",
    "            all_union = top_suggested_items\n",
    "        else:\n",
    "            all_union = all_union.union(top_suggested_items)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error occur for {} : {}\".format(user, e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec@k score: 0.10204222222222223\n",
      "\n",
      "intersect\n",
      "set() 0\n",
      "\n",
      "union\n",
      "{15106, 4354, 11529, 3594, 5129, 14604, 11533, 8975, 11536, 13591, 2073, 1567, 5668, 550, 1321, 10796, 8750, 1839, 2606, 8497, 7476, 12342, 3135, 65, 13122, 8259, 8001, 8517, 10050, 4681, 3401, 1611, 3663, 3921, 13906, 594, 14932, 14162, 12378, 10330, 13405, 1886, 9056, 13152, 6247, 12394, 7027, 628, 8052, 9334, 10103, 7286, 8823, 8317, 9342, 12926, 7304, 12681, 2187, 5260, 911, 13712, 1168, 4499, 10646, 15258, 2972, 11677, 7074, 10659, 676, 1187, 13224, 6825, 2223, 433, 8115, 14334, 14774, 14522, 13243, 5058, 8389, 14025, 14795, 3788, 11981, 8918, 8662, 4827, 13276, 13277, 11230, 13535, 14560, 9441, 9443, 5348, 11238, 11497, 4586, 1003, 491, 13551, 13310, 9727} 106\n",
      "\n",
      "distinct rate\n",
      "0.00212\n"
     ]
    }
   ],
   "source": [
    "print(\"Prec@k score:\", np.average(scores))\n",
    "# print(\"top_suggested_items:\", top_suggested_items)\n",
    "# print(\"truth_items:\", truth_items)\n",
    "\n",
    "print(\"\\nintersect\")\n",
    "print(all_intersect, len(all_intersect))\n",
    "print(\"\\nunion\")\n",
    "print(all_union, len(all_union))\n",
    "print(\"\\ndistinct rate\")\n",
    "print((len(all_union)) / (n_users * k_suggestion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119468\n",
      "[(2.7601897716522217, 5058), (2.7598536014556885, 10659), (2.462766170501709, 7304), (2.1554150581359863, 2073), (2.124174118041992, 13122), (2.118602991104126, 14025), (2.1142380237579346, 11536), (2.1131255626678467, 6825), (2.1125900745391846, 1321), (2.1121749877929688, 911)]\n",
      "[5058, 10659, 7304, 2073, 13122, 14025, 11536, 6825, 1321, 911]\n",
      "[(5.0, 14403), (5.0, 14162), (5.0, 12475), (5.0, 10659), (5.0, 5058), (5.0, 3401), (5.0, 3264), (5.0, 2903), (5.0, 2897), (5.0, 1392)]\n",
      "({5058, 10659}, 0.2)\n",
      "==================\n",
      "6491\n",
      "[(1.918827772140503, 5058), (1.9182579517364502, 10659), (1.6212118864059448, 7304), (1.313458800315857, 2073), (1.283602237701416, 13122), (1.2776063680648804, 14025), (1.273353099822998, 11536), (1.2721185684204102, 6825), (1.2716999053955078, 1321), (1.2710808515548706, 10796)]\n",
      "[5058, 10659, 7304, 2073, 13122, 14025, 11536, 6825, 1321, 10796]\n",
      "[(5.0, 10659), (5.0, 8962), (5.0, 5058), (5.0, 2073), (5.0, 1168), (5.0, 340), (4.5, 12536), (4.5, 7304), (4.5, 6930), (4.5, 2633)]\n",
      "({7304, 2073, 5058, 10659}, 0.4)\n",
      "==================\n",
      "117214\n",
      "[(2.8766448497772217, 5058), (2.8761467933654785, 10659), (2.5791025161743164, 7304), (2.271371841430664, 2073), (2.2410011291503906, 13122), (2.2354512214660645, 14025), (2.231513023376465, 11536), (2.2298660278320312, 6825), (2.2296552658081055, 1321), (2.2292096614837646, 10796)]\n",
      "[5058, 10659, 7304, 2073, 13122, 14025, 11536, 6825, 1321, 10796]\n",
      "[(5.0, 11536), (5.0, 10750), (5.0, 10659), (5.0, 9113), (4.0, 14162), (4.0, 13122), (4.0, 7304), (4.0, 5425), (4.0, 5058), (4.0, 3870)]\n",
      "({7304, 5058, 13122, 10659, 11536}, 0.5)\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "sample_user = [np.random.randint(1, 138000) for i in range(0, 3)]\n",
    "\n",
    "for user in sample_user:\n",
    "\n",
    "    prediction = get_top_suggestion(user, 10)\n",
    "    truth = get_top_truth(user, 10)\n",
    "\n",
    "    print(user)\n",
    "    print((prediction))\n",
    "    print([x[1] for x in prediction])\n",
    "    print((truth))\n",
    "    print(check_precision(prediction, truth, 10))\n",
    "    print(\"==================\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
